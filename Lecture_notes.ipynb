{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1 Introduction\n",
    "\n",
    "1. Supervised learning \n",
    "    - regression(continous value)\n",
    "    - classification\n",
    "    pb: how to present a poin that lies in an infinite dimensional space.\n",
    "\n",
    "2. Learning theory\n",
    "    explain how the algorithms do the works and garantee the accuracy, to know the pattern\n",
    "\n",
    "3. Unsupervised learning\n",
    "    - a single image to do 3d reconstruction with clustering\n",
    "    - cocktail party problem(source seperation question): ICA in EEG data\n",
    "\n",
    "4. Reinforcement learning\n",
    "    doing the whole sequence over time of making a good/woring decision instead of one short decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2 An application of Supervised learining\n",
    "\n",
    "## 1. Linear regression\n",
    "preducting a continous value like steering direction of autonomous cars.\n",
    "\n",
    "X1: Size(feet^2) | X2: Nb of rooms | Y: price($)\n",
    " --- | --- | --- | \n",
    "2104 | 2 | 400\n",
    "\n",
    "\n",
    "m = training example  \n",
    "x = input variables/feature  \n",
    "y = output variable/target  \n",
    "(x, y) training example  \n",
    "$i^{th}$ training exampele ($x^{i}, y^{i}$)\n",
    "\n",
    "### 1.1 Hypothesis and model\n",
    "   - define a hypothesis **H** : maps from input X to outputs Y.\n",
    "   - define a learning algorithm for **H** : $ H(X) = \\theta_0 + \\theta_1*X$\n",
    "   \n",
    "$$h(x) = h_{\\Theta}(x) = \\Theta_0 + \\Theta_1*x_1 + \\Theta_2*X_2 $$ where ${\\theta}$ is the paremeter of model.\n",
    "\n",
    "for conciseness, define $X_0$ = 1\n",
    "\n",
    "$$ h(x^j) = \\sum_{i = 0}^2 \\Theta_ix_i^j = \\Theta^Tx^j $$\n",
    "\n",
    "* define a cost function:\n",
    "$$ J(\\Theta) = \\frac{1}{2} \\sum_{i=1}^m (h_\\theta(x^i) - y^i)^2 = \\frac{1}{2} (h_\\theta(X) - Y)^2$$\n",
    "* objectif : $$\\underset{\\Theta}{\\min} J(\\Theta) $$\n",
    "\n",
    "### 1.2 Fit parameters $\\Theta$ : Gradient descent\n",
    "\n",
    "- start with some $\\Theta$ (Say $\\Theta = \\vec{0} $)\n",
    "- then, updating $\\Theta$ to reduce J($\\Theta$) with learning rule:\n",
    "\n",
    "$$\\Theta_i := \\Theta_i - \\alpha \\frac{\\partial}{\\partial \\Theta_i} J(\\Theta) $$  \n",
    "with $\\alpha$ is learing rate which present the learning speed (hyperparameter set by hand)\n",
    "\n",
    "- for one example:\n",
    "$$\\frac{\\partial}{\\partial \\Theta_i} J(\\Theta) = \\frac{\\partial}{\\partial \\Theta_i} \\frac{1}{2} (h_\\theta(x) - y)^2 = (h_\\theta(x) - y)* x_i$$\n",
    "finally,we get \n",
    "$$\\Theta_i := \\Theta_i - \\alpha * (h_\\theta(x) - y)* x_i $$  \n",
    "\n",
    "- for m example, repeat until convergence:\n",
    "$$\\Theta_i := \\Theta_i - \\alpha \\frac{\\partial}{\\partial \\Theta_i}\\frac{1}{2} \\sum_{j}^m (h_\\theta(x^j) - y^j)^2 = \\Theta_i - \\alpha \\sum_{j=1}^m (h_\\theta(x^j) - y^j)*x_i^j $$  \n",
    "\n",
    "the above **Batch Gradient Descent :** in each iteration, we gonna use the entire dataset\n",
    "one altenative method is **Stochastic Gradient Descent :** incremental form which is much faster and converges to the global optimum exactly. \n",
    "\n",
    "repeat \\{  \n",
    "    for j = 1 to m \\{  \n",
    "    for all values of i  \n",
    "        $$\\Theta_i: = \\Theta_i - \\alpha (h_\\theta(x^j) - y^j)*x_i^j $$   \n",
    "    \\}  \n",
    "\\}\n",
    "\n",
    "### 1.3 Normal equation\n",
    "$f : \\mathbb{R}^{m\\times n} \\mapsto \\mathbb{R}$  \n",
    " tr AB = tr BA  \n",
    " tr ABC = tr CAB = tr BCA  \n",
    "$\\triangledown_A tr AB = B^T $  \n",
    " $\\triangledown_A tr ABA^TC = CAB + C^TAB^T $   \n",
    " tr A = tr$ A^T$  \n",
    " \n",
    "  $$\\triangledown_{\\Theta} J(\\Theta) = \\triangledown_{\\Theta} \\frac{1}{2} (X\\Theta - y)^T(X\\Theta - y)  \n",
    "  =\\triangledown_{\\Theta} \\frac{1}{2} tr (X\\Theta - y)^T(X\\Theta - y) $$\n",
    "  $$= X^TX\\Theta - X^Ty \\Rightarrow 0$$\n",
    "  \n",
    "  \n",
    "  we get Normal equations: $$X^TX\\Theta = X^Ty $$  \n",
    "  the value of θ that minimizes J(θ) is given in closed form:\n",
    "  $$\\Theta =(X^TX)^{-1}X^Ty$$\n",
    "  \n",
    "Linear regression:  \n",
    "to minimize J($\\theta$) and return $\\theta^TX$\n",
    "\n",
    " More information: https://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression\n",
    " \n",
    " ### 1.4 local weight regression\n",
    "Linear regression:  \n",
    "to minimize J($\\theta$) and return $\\theta^TX$\n",
    "\n",
    "LWR: fit $\\theta$ to minimize:   \n",
    "$$\\sum_{i=1}^m w^i * (h_\\theta(x^i) - y^i)^2$$\n",
    "where $w^i = exp(-\\frac{(x^i-x)^2}{2}) \\in [0,1]$  \n",
    "or, $w^i = exp(-\\frac{(x^i-x)^2}{2\\tau}) \\in [0,1]$ and $\\tau $ is bandwidth and controls the dispersion like gaussian distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Probabilistic interpretation\n",
    "Assume $y^i = \\Theta^TX^i + \\epsilon^i$ where $\\epsilon $ is random noise  \n",
    "$\\epsilon^i \\hookrightarrow  \\mathcal{N}(0,\\,\\sigma^{2})  $  \n",
    "\n",
    "$$ P(\\epsilon^i) = \\frac{1}{{\\sigma \\sqrt {2\\pi}}} e^{- {\\epsilon^i}^2/{2\\sigma ^2 }}$$\n",
    "$y^i \\hookrightarrow  \\mathcal{N}(\\Theta^ix^i,\\,\\sigma^{2}),  \\epsilon^i = y^i - \\Theta^TX^i$ , so:\n",
    "$$ P(y^i|x^i,\\Theta) = \\frac{1}{{\\sigma \\sqrt {2\\pi}}} e^{- ({y^i -\\Theta^Tx^i})^2/{2\\sigma ^2 }}$$\n",
    "why using gaussian? central limite theorem: adding some independent variables, the result return to gaussian.\n",
    "$\\epsilon^i$ is iid. independently identically distributed:\n",
    "###### Likelihood \n",
    "$$L(\\Theta) =  P(\\vec{y}|X; \\Theta) = \\prod_{i=1}^{m}P(y^i|x^i,\\Theta) = \\prod_{i=1}^{m} \\frac{1}{{\\sigma \\sqrt {2\\pi}}} e^{- ({y^i -\\Theta^Tx^i})^2/{2\\sigma ^2 }}$$ \n",
    "###### Maximun likelihood\n",
    "$$\\Theta^* = \\underset{\\Theta}{\\arg\\max} L(\\Theta) = \\underset{\\Theta}{\\arg\\max} \\log L(\\Theta) = \\underset{\\Theta}{\\arg\\max} \\sum_{i=1}^{m} - ({y^i -\\Theta^Tx^i})^2/{2\\sigma ^2 } = \\underset{\\Theta}{\\arg\\min} \\sum_{i=1}^{m} ({y^i -\\Theta^Tx^i})^2/{2\\sigma ^2 } = {\\arg\\min} J(\\Theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
